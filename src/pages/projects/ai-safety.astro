---
import Layout from "@/layouts/Layout.astro";
---

<Layout title="AI Safety & Alignment | Harutyun Harry Ilanyan">
  <div class="prose max-w-none">
    <h1 class="text-3xl font-bold mb-8">AI Safety Calibration Framework</h1>
    
    <div class="alert alert-success mb-8">
      <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" class="stroke-current shrink-0 w-6 h-6">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path>
      </svg>
      <span>This project is actively advancing AI safety research with concrete applications.</span>
    </div>

    <h2>Project Overview</h2>
    <p>
      Comprehensive tools for measuring and improving model calibration with focus on toxicity mitigation, alignment, 
      and safety evaluation using TruthfulQA and RealToxicityPrompts datasets. This framework ensures AI systems behave 
      reliably and beneficially through advanced safety techniques.
    </p>

    <h2>Key Components</h2>
    <ul>
      <li>Model calibration measurement and improvement tools</li>
      <li>Toxicity mitigation strategies</li>
      <li>Alignment-aware training methodologies</li>
      <li>Comprehensive safety evaluation frameworks</li>
      <li>Integration with TruthfulQA and RealToxicityPrompts</li>
    </ul>

    <h2>Technical Stack</h2>
    <div class="flex flex-wrap gap-2 mb-6 not-prose">
      <div class="badge badge-primary">Python</div>
      <div class="badge badge-secondary">Transformers</div>
      <div class="badge badge-accent">AI Safety</div>
      <div class="badge badge-info">Model Evaluation</div>
      <div class="badge badge-success">Calibration</div>
    </div>

    <h2>Research Impact</h2>
    <p>
      This framework contributes to building more trustworthy AI systems by providing robust tools for safety evaluation 
      and ensuring that models behave in alignment with human values and expectations.
    </p>

    <h2>Evaluation Datasets</h2>
    <ul>
      <li><strong>TruthfulQA:</strong> Measuring truthfulness in language model responses</li>
      <li><strong>RealToxicityPrompts:</strong> Evaluating and mitigating toxic language generation</li>
      <li><strong>Custom Safety Benchmarks:</strong> Specialized evaluation metrics for alignment</li>
    </ul>

    <div class="flex gap-4 mt-8 not-prose">
      <a href="/projects" class="btn btn-outline">‚Üê Back to Projects</a>
      <a href="mailto:harry_ila@berkeley.edu" class="btn btn-primary">Contact for Collaboration</a>
    </div>
  </div>
</Layout>